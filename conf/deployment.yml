# Custom section is used to store configurations that might be repetative.
# Please read YAML documentation for details on how to use substitutions and anchors.
custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "13.0.x-scala2.12"
    node_type_id: "i3.xlarge"
    autoscale:
      min_workers: 1
      max_workers: 2

  basic-static-cluster: &basic-static-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 1
      node_type_id: "i3.xlarge"

build:
  no_build: true
  #python: "poetry"

environments:
  default:
      workflows:
      ######################################################################################
      # Example workflow for integration tests                                            #
      ######################################################################################
      - name: "databricks_with_poetry-sample-tests"
        job_clusters:
        - job_cluster_key: "basic-cluster"
          <<: *basic-static-cluster
        
        tasks:
          - task_key: "main"
            job_cluster_key: "basic-cluster"
            spark_python_task:
                python_file: "file://tests/entrypoint.py"
                # this call supports all standard pytest arguments
                parameters: ["file:fuse://tests/integration", "--cov=databricks_with_poetry"]

    # workflows:
      ######################################################################################
      # Example workflow for integration tests                                            #
      ######################################################################################
      # - name: "databricks_with_poetry-sample-tests"
      #   tasks:
      #     - task_key: "main"
      #       job_cluster_key: "default"
      #       spark_python_task:
      #           python_file: "file://tests/entrypoint.py"
      #           # this call supports all standard pytest arguments
      #           parameters: ["file:fuse://tests/integration", "--cov=databricks_with_poetry"]
      # #######################################################################################
      # # this is an example job with single ETL task based on 2.1 API and wheel_task format #
      # ######################################################################################
      # - name: "databricks_with_poetry-sample-etl"
      #   tasks:
      #     - task_key: "main"
      #       <<: *basic-static-cluster
      #       python_wheel_task:
      #         package_name: "databricks_with_poetry"
      #         entry_point: "etl" # take a look at the setup.py entry_points section for details on how to define an entrypoint
      #         parameters: ["--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml"]
      # #############################################################
      # # this is an example multitask job with notebook task       #
      # #############################################################
      # - name: "databricks_with_poetry-sample-multitask"
      #   job_clusters:
      #     - job_cluster_key: "default"
      #       <<: *basic-static-cluster
      #   tasks:
      #     - task_key: "etl"
      #       job_cluster_key: "default"
      #       spark_python_task:
      #         python_file: "file://databricks_with_poetry/tasks/sample_etl_task.py"
      #         parameters: [ "--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml" ]
      #     - task_key: "ml"
      #       depends_on:
      #         - task_key: "etl"
      #       job_cluster_key: "default"
      #       python_wheel_task:
      #         package_name: "databricks_with_poetry"
      #         entry_point: "ml"
      #         parameters: [ "--conf-file", "file:fuse://conf/tasks/sample_ml_config.yml" ]
          ###############################################################################
          # this is an example task based on the notebook                               #
          # Please note that first you'll need to add a Repo and commit notebook to it. #
          ###############################################################################
          # - task_key: "notebook"
          #   deployment_config:
          #     no_package: true # we omit using package since code will be shipped directly from the Repo
          #   # depends_on:
          #   #   - task_key: "ml"
          #   job_cluster_key: "default"
          #   notebook_task:
          #     notebook_path: "/Users/dustin.vannoy@databricks.com/databricks_with_poetry/notebooks/sample_notebook"

